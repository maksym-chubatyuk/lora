# JAX for TPU
jax[tpu]>=0.4.30
jaxlib>=0.4.30

# EasyDeL for LoRA training
easydel>=0.0.80

# JAX ecosystem
optax>=0.2.0
flax>=0.8.0
orbax-checkpoint>=0.5.0

# Data loading
datasets>=2.18.0
grain>=0.2.0

# Tokenizer
transformers>=4.40.0

# Utilities
numpy>=1.24.0
tqdm

# For GGUF export (run on CPU machine after training)
# torch>=2.1.0
# See: https://github.com/ggerganov/llama.cpp
